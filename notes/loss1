embedding_dims = 300
i_vec = torch.nn.Embedding(vocabulary_size, embedding_dims, padding_idx=0).float()
o_vec = torch.nn.Embedding(vocabulary_size, embedding_dims, padding_idx=0).float()
num_epochs = 100
learning_rate = 0.025
batch_size = len(idx_pairs)
negative_samples = 25
weight_dist = create_distribution(idx_pairs)

torch.manual_seed(123)

i_vec.weight = torch.nn.Parameter(torch.cat([torch.zeros(1, embedding_dims), 
                                       torch.FloatTensor(vocabulary_size - 1, 
                                          embedding_dims).uniform_(-0.5 / embedding_dims, 0.5 / embedding_dims)]))
o_vec.weight = torch.nn.Parameter(torch.cat([torch.zeros(1, embedding_dims), 
                                       torch.FloatTensor(vocabulary_size - 1, 
                                          embedding_dims).uniform_(-0.5 / embedding_dims, 0.5 / embedding_dims)]))


i_vec.weight.requires_grad = True
o_vec.weight.requires_grad = True

i_vectors = torch.LongTensor()
o_vectors = torch.LongTensor()

optimizer = torch.optim.Adam([i_vec.weight, o_vec.weight], lr=learning_rate)

loss_val = 0

for epoch in range(num_epochs):
    print(f"Epoch:{epoch}")
    i_words = []
    o_words = []
    n_words = []

    for idx, words in enumerate(idx_pairs, 1):
        i_words.append(words[0])
        o_words.append(words[1])

        if idx % batch_size == 0:
            
            i_vectors = i_vec(torch.LongTensor(i_words)).unsqueeze(2)
            o_vectors = o_vec(torch.LongTensor(o_words))
            
            l_vectors = torch.bmm(o_vectors, i_vectors).squeeze()

            #lloss = torch.bmm(l_vectors.unsqueeze(1), o_vectors.sigmoid().log().mean(2).unsqueeze(2))
            #lloss = torch.bmm(l_vectors.unsqueeze(1), o_vectors.mean(2).unsqueeze(2))
            lloss = F.log_softmax(l_vectors, dim=1)
            #print(lloss.shape)
            oloss = o_vectors.detach().sigmoid().log().mean(2).mean(1).long()
            #print(oloss.data)
            loss = F.nll_loss(lloss, oloss)

            #loss = lloss.sigmoid().log().mean().neg()
            #loss = lloss.mean().neg()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
            loss_val += loss

            i_words = []
            o_words = []
            n_words = []

    print("loss: " + str(float(loss_val.data/(len(idx_pairs)/batch_size))))
    loss_val = 0
